{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TO-DO: aquest programa dóna un error amb les dimensions de la imatge que es passa a pytorch. Si es resol aquest error, el learner hauria de funcionar correctament."
      ],
      "metadata": {
        "id": "zkJb-vI__oiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En aquest últim apartat extra veurem un exemple de reinforcement learning amb un agent constituit per una xarxa neuronal convolucional. <br>\n",
        "\n",
        "Cal notar que aquest exemple s'ha intentat construir, sense èxit, amb la llibreria ``fastai`` que hem estat fent servir durant la pràctica. No obstant, hem tingut problemes per crear el dataloader (com veurem, en el reinforcment learning no fem servir un conjunt d'entrenament predefinit, sinó que anem entrenant \"sobre la marxa\" a mesura que l'agent interactua amb l'entorn), així que hem optat per treballar directament amb Pytorch."
      ],
      "metadata": {
        "id": "ouN00LWzZqD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[accept-rom-license]\n",
        "!pip install gym[atari]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBlbH2DibRTp",
        "outputId": "c25920cb-1fc7-4fc6-cba8-b573ab3b0732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[accept-rom-license] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license]) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license]) (6.3.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license]) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[accept-rom-license]) (1.22.4)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.27.1)\n",
            "Collecting AutoROM.accept-rom-license\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license]) (3.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.4)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446676 sha256=dbf1b0f1f6fdf221c2225f91a2956be58f4a02f2125312f4c66dd2c8eb7ea343\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/1f/f7/2da07cf4f81ea264bdaf043028749d88fe0c2227134a22cf80\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym[atari]) (6.3.0)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.9/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.15.0)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtTtMK44Zkmi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a22cd718-1faa-42e1-8532-0ac1d82ec20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"BreakoutDeterministic-v4\").env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rGgSHi-ebEfn",
        "outputId": "e3eacd8f-a297-4dc0-9af8-ca0efd6ef366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num. accions:\", env.action_space.n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KedAhpolcvQV",
        "outputId": "bf40356d-bf44-4e22-f807-90b619ce894b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num. accions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordem: accions són SERVIR, NO FER RES, DRETA i ESQUERRA."
      ],
      "metadata": {
        "id": "Mq_MgbB3ol_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anem a crear una xarxa neuronal que actuarà com a *cervell* del nostre agent intel·ligent. Donada una observació (en el nostre cas corresponent a un estat), aquesta xarxa s'ocuparà d'estimar la utilitat de les parelles estat-acció:"
      ],
      "metadata": {
        "id": "1KQ-aOjQerZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentBrain(nn.Module):\n",
        "  def __init__(self, lr_input):\n",
        "\n",
        "    super(AgentBrain, self).__init__()\n",
        "\n",
        "\n",
        "\n",
        "    self.conv1 = nn.Conv2d(3, 32, 3) # 208x158x32\n",
        "    self.relu1 = nn.ReLU(inplace = True)\n",
        "    self.conv2 = nn.Conv2d(32, 64, 3) # 206x156x64\n",
        "    self.relu2 = nn.ReLU(inplace = True)\n",
        "    self.pool1 = nn.MaxPool2d(2, 2) # 103x78x64\n",
        "\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3) #101x76x128\n",
        "    self.relu3 = nn.ReLU(inplace = True)\n",
        "    self.conv4 = nn.Conv2d(128, 256, 3) #99x74x256\n",
        "    self.relu4 = nn.ReLU(inplace = True)\n",
        "    self.pool2 = nn.MaxPool2d(2, 2) #49x37x256\n",
        "\n",
        "    self.fc1 = nn.Linear(49 * 37 * 256, 64)\n",
        "    self.relu5 = nn.ReLU(inplace = True)\n",
        "    self.fc2 = nn.Linear(64, 4) #num. accions és 4, com hem vist abans.\n",
        "\n",
        "    self.device = torch.device('cuda')\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr = lr_input)\n",
        "    self.loss = nn.MSELoss()\n",
        "    self.to(self.device)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    \n",
        "    x = self.conv1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = self.conv3(x)\n",
        "    x = self.relu3(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.relu4(x)\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu5(x)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "xDD9Y1Ftcqsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construim ara l'agent. A"
      ],
      "metadata": {
        "id": "vedXFbqS_5eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "  def __init__(self, gamma, epsilon, lr, input_dims, batch_size, max_memory_size = 10000, eps_end = 0.01, eps_dec = 5e-4):\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.eps_min = eps_end\n",
        "    self.eps_dec = eps_dec\n",
        "\n",
        "    self.lr = lr\n",
        "\n",
        "    self.action_space = [0, 1, 2, 3]\n",
        "\n",
        "    self.mem_size = max_memory_size\n",
        "    self.mem_counter = 0\n",
        "\n",
        "\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "    self.Q_eval = AgentBrain(lr)\n",
        "\n",
        "    self.state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32)\n",
        "    self.new_state_memory = np.zeros((self.mem_size, *input_dims), dtype = np.float32)\n",
        "\n",
        "    self.action_memory = np.zeros(self.mem_size, dtype=np.int32)\n",
        "    self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
        "    self.terminal_memory = np.zeros(self.mem_size, dtype = np.bool_)\n",
        "\n",
        "\n",
        "  def store_transition(self, state, action, reward, next_state, is_done):\n",
        "\n",
        "    next_state = np.moveaxis(next_state, -1, 0)\n",
        "    state = np.moveaxis(state, -1, 0)\n",
        "\n",
        "    i = self.mem_counter % self.mem_size\n",
        "\n",
        "    self.state_memory[i] = state\n",
        "    self.new_state_memory[i] = next_state\n",
        "    self.reward_memory[i] = reward\n",
        "    self.action_memory[i] = action\n",
        "    self.terminal_memory[i] = is_done\n",
        "\n",
        "    self.mem_counter += 1\n",
        "\n",
        "\n",
        "  def choose_action(self, obs):\n",
        "\n",
        "    random_number = np.random.random()\n",
        "\n",
        "    if random_number > self.epsilon:\n",
        "      state = torch.tensor([obs]).to(self.Q_eval.device)\n",
        "      action = torch.argmax(self.Q_eval.forward(state)).item()\n",
        "\n",
        "    else:\n",
        "      action = np.random.choice(self.action_space)\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "  def learn(self):\n",
        "    \n",
        "    if self.mem_counter < self.batch_size:\n",
        "      return\n",
        "\n",
        "    self.Q_eval.optimizer.zero_grad()\n",
        "\n",
        "    max_mem = min(self.mem_counter, self.mem_size)\n",
        "\n",
        "    batch_exemples = np.random.choice(max_mem, self.batch_size, replace = False)\n",
        "    batch_index = np.arange(self.batch_size, dtype = np.int32)\n",
        "\n",
        "    disp = self.Q_eval.device\n",
        "\n",
        "    state_batch = torch.tensor(self.state_memory[batch_exemples]).to(disp)\n",
        "    next_state_batch = torch.tensor(self.new_state_memory[batch_exemples]).to(disp)\n",
        "    reward_batch = torch.tensor(self.reward_memory[batch_exemples]).to(disp)\n",
        "    terminal_batch = torch.tensor(self.terminal_memory[batch_exemples]).to(disp)\n",
        "    action_batch = self.action_memory[batch_exemples]\n",
        "\n",
        "    q_eval = self.Q_eval.forward(state_batch)[batch_index, action_batch]\n",
        "    q_next = self.Q_eval.forward(next_state_batch)\n",
        "    q_next[terminal_batch] = 0.0\n",
        "\n",
        "    q_target = reward_batch + self.gamma * torch.max(q_next, dim = 1)[0]\n",
        "\n",
        "    loss = self.Q_eval.loss(q_target, q_eval).to(disp)\n",
        "    loss.backward()\n",
        "    self.Q_eval.optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udKxdxjcnO4y",
        "outputId": "83c0d5c6-d98f-4bf8-c980-b738fd8a3908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creem ara una instància de l'agent:"
      ],
      "metadata": {
        "id": "mSQ14xxLANhB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(gamma = 0.99, epsilon = 1.0, lr = 1e-5, input_dims = (3, 210, 160), batch_size = 64)"
      ],
      "metadata": {
        "id": "OKy-PQLauuTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalment creem el bucle principal del programa:"
      ],
      "metadata": {
        "id": "3N6KSgTRAPpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "\n",
        "for i in range(10):\n",
        "  score = 0\n",
        "  done = False\n",
        "  obs = env.reset()\n",
        "  #obs = np.moveaxis(obs, -1, 0)\n",
        "\n",
        "  while not done:\n",
        "    action = agent.choose_action(obs)\n",
        "    obs_next , reward, terminated, _ = env.step(action)\n",
        "    #obs_next = np.moveaxis(obs_next, -1, 0)\n",
        "\n",
        "    score += reward\n",
        "\n",
        "    done = terminated\n",
        "\n",
        "    agent.store_transition(obs, action, reward, obs_next, terminated)\n",
        "\n",
        "    agent.learn()\n",
        "\n",
        "    obs = obs_next\n",
        "\n",
        "  scores.append(score)\n",
        "\n",
        "  avg_score = np.mean(scores[-100:])\n",
        "\n",
        "  print(\"Ep.\", i, \"score %.3f\" % score, 'avg. score %.3f' % avg_score)\n",
        "\n"
      ],
      "metadata": {
        "id": "UDe6qvNFw1nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hauríem de veure com a cada iteració l'score va aumentant."
      ],
      "metadata": {
        "id": "quA1ecU8AYvQ"
      }
    }
  ]
}